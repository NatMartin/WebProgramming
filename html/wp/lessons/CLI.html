<!DOCTYPE HTML>
<html ng-app="wp">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/css/bootstrap.min.css" type="text/css">
    <link rel="stylesheet" href="/css/main.css" type="text/css">
    <title>WP</title>
  </head>

  <!-- Lessons -->
  <body ng-controller="cliCtrl">

    <!-- Lessons Navigation Bar -->
    <div ng-include="'templates/navigation.tpl.html'"></div>

    <!-- Lessons Jumbotron -->
    <div ng-include="'/templates/head.tpl.html'"></div>

    <!-- Lessons Body -->
    <div class="container">
      <div class="row">

	<!-- Lessons Table of Content -->
	<div class="col-md-5">
	  <div ng-include="'templates/toc.tpl.html'"></div>
	</div>

	<!-- Lesson: Introduction -->
	<div class="col-md-7 wp-text-area">
	  <h1>The Birth of Computer Software</h1>
	  <p>
	    Until 1946, computers were programmed by rewiring
	    them. Turing had shown how, in principle, a computer could
	    be created that could emulate any other computer ten years
	    earlier, but as a practical matter, the machine were too
	    slow to emulate anything. The cost and slow speed of the
	    hardware forced programmers to manipulate the hardware
	    itself to change the behavior of the program. To modern
	    application developers this seems about as reasonable as
	    teaching children reading through brain surgery.
	  </p>
	  <p>
	    The situation improved with the development of stored
	    program computers. These computers use their memory as the
	    repository for the sequence of instructions they will
	    follow. The Central Processing Unit (CPU) pick an
	    operation from memory: an arithmetic operation (add,
	    subtract, multiply or divide), a store operation, a read
	    operation, or a jump operation (execute a sequence of
	    operations starting in a new location based on some
	    condition). These computers approximate Turing's universal
	    computer since their behavior is completely determined by
	    the contents of their memory.
	  </p>
	  <p>
	    The first programs were written in machine language: a
	    long string of binary digits which the computer would
	    interpret as either operator or operands. The operators
	    told the computer want to do, such a add the values in
	    memory location specified by the operands or to start
	    executing the program starting with a new location in
	    memory specified by the operand. This is called machine
	    code and it is still the only language the CPU can
	    execute.  Programs in machine code were often entered into
	    computer memory by flipping switches that altered each
	    individual bit.
	  <p>
	    Computers execute programs faster than people enter
	    program into memory, so engineers attached card
	    readers to the computer so the programmers could write the
	    program onto cards, then have the card reader enter the
	    program into the computer's memory.
	  </p>
	  <h2>Operating Systems and Time Sharing</h2>
	  <p>
	    Unfortunately reading programs into memory and printing
	    out results takes much longer than running the programs:
	    the multi-million dollar computer must for a
	    multi-thousand dollar card-reader or printer to
	    finish.
	  </p>
	  <p>
	    Batch programming solved the problem of reading programs
	    into memory. Since the CPU only looks at a single
	    instruction at a time, the card reader can put
	    instructions anywhere except few spots the computer needs
	    during the time the card reader's operation. The computer
	    can jump to the next program as soon as it finishes the
	    current one and the card read can start putting the next
	    program in the location of the finished program.
	  </p>
	  <p>
	    Monitors provide a similar solution to the problem of
	    printing. A monitor is a program that stops the execution
	    of one program when output needs to be printed and starts
	    another program. When the new program prints, yet another
	    starts. Eventually, the computer cycles around to the
	    original program and start it again.
	  </p>
	  <p>
	    Programmers quickly realized that they could also switch
	    between running programs for any reason. Keeping track of
	    the various elements that needed to be printed and
	    arranging the programs that were being read into memory
	    both required the CPU, so the computer would need to jump
	    back and forth between reading, writing, and doing
	    work. Arranging the programs in a regular fashion and
	    jumping between them shorten the calculations need to
	    decide where to put the new program or print jobs. Indeed,
	    the computer can easily move between programs that do work
	    for the users as it can between programs that manage the
	    use of the computer.
	  </p>
	  <p>
	    One of the programs developed to help programmers was the
	    assembler. I translates code that represent operators and
	    operands into the binary representation of those
	    codes. For example to load the accumulator, one might type
	    <code>LDA 10</code>. The accumulator is a particular
	    memory location that holds one of the operands for a two
	    operand operation and the result of that operation after
	    its execution. To then add 5 to the contents of the
	    accumulator one might write <code>ADD 5</code>. To then
	    store the results into memory location 10, one might
	    write <code>STO 10</code>. In a modern programming
	    langague this might be represented as <code>x = x +
	    5</code>. The accumulator might translate:
	    <div class="code">
LDA 10
ADD 5
STO 10
	    </div>
	    into
	    <div class="code">
001010110000101000111011000000110011101100001010
	    </div>
	    Though the assembly language is obscure, it is a great
	    improvement over machine code. My father told me that when
	    he was writing programs in the 1950's, programmers
	    complained tha assembly language made programming too
	    easy, because now anyone could do it.
	  </p>
	  <p>
	    As more devices, such a disk drives or tape drives, which
	    are also much slower than the CPU were added to computer
	    systems, managing input and output became a significant
	    portion of the computer's work. In addition, more programs
	    like assemblers and editor were adding to the computer
	    burden. The work of managing devices and programs was
	    collected into a single program that is now called the
	    Operating System. The Operating System manages the devices
	    connected to the computer and the programs the computer is
	    running. It abstracts away the hardware, so programmers
	    are actually programming the operating system when they
	    program computers.
	  </p>
	  <p>
	    The operating system embody Turing's insight that programs
	    easily simulate any physical process, but the computer is
	    still basically a device for doing complicated
	    calculations using the model that dates back to
	    Babbage. The programmer puts the program and data into
	    memory, then tells the CPU to begin execution of the
	    program at a particular memory location.
	  </p>
	  <h3>Time Sharing</h3>
	  <p>
	    Operating systems become the programming
	    environment. Programmers may still write machine
	    instructions that the computer will march through, by the
	    sequence of these instructions is significantly modified
	    by the operating system. The operating system manages the
	    computer's memory, giving a particular section of it to
	    each program. It can make the memory look larger than it
	    is using a technique called virtual memory, which loads a
	    part of a program into memory just before the computer
	    need to execute it, and moving the completed portion out
	    of memory. It can make input and output much faster by
	    continuing to execute programs while it is waiting. It can
	    make a disk drive look like an organized set of files
	    instead of a long string of binary digits.
	  </p>
	  <p>
	    However, most startling, the operating system can make a
	    single computer look like many computers. It does this
	    through time sharing. Time sharing loads multiple programs
	    into memory and runs the first for a few seconds. When the
	    time is up it moves to the next program and runs it for a
	    few seconds. It continues cycling through the programs in
	    memory until one of the programs completes. When a program
	    completes, it uses that program's memory to load a new
	    program in. Time sharing makes it look like each program
	    has its own computer.
	  </p>
	  <p>
	    The principle of making a single computer look like
	    multiple computers extends to input and output devices. If
	    a program needs to print, it request printing from the
	    operating system. The operating system then checks to see
	    if the printer is busy and, if it is not, it starts the
	    printer and pauses the program until the printing is
	    complete. It continue cycling through the programs that
	    have not paused continuing to do work for the other
	    computer users.
	  </p>
	  <p>
	    The same thing can be done with the card reader entering
	    the program. It can start loading a program into memory
	    until that programs time slot is used up and move to the
	    next one already in memory. Eventually, the program will
	    be loaded in memory and the computer can start executing
	    it.
	  </p>
	  <h2>Teletype and terminal</h2>
	  <p>
	    Because input and output no longer hold up the execution
	    of other programs, input and output can be done in smaller
	    increments. Initially, programs were written so that all
	    of the input happened at the beginning and all output
	    happened at the end. Since the operating system manages
	    input and output so that it does not disturb other
	    programs, input and output can be done at any time. Since
	    the input and output can be done at any time, the computer
	    can begin to converse with the programmer providing
	    interim reports on its operating and allowing the
	    programmer to alter its working based on those reports.
	  </p>
	  <h3>Teletype and Terminal</h3>
	  <p>
	    The teleprinter (which is usually called at teletype
	    machine from the Teletype company that made the first
	    ones) comprises a typewriter and a communications
	    medium. It lets the user type on it and, as the user
	    types, it sends electronic signals that activate the same
	    keys on another teletype machine. In this way, teletype
	    users can sent typed messages.
	  </p>
	  <img>
	  <div class="illustration">
	    <img src="img/teletype.jpg" alt="Teletype">
	    <div class="caption">
	      Teletype Model 33 ASR teleprinter with punched tape
	      reader and punch usable as a computer terminal.
	    </div>
	    <div class="imgref">
	      <a href="http://commons.wikimedia.org/wiki/File:ASR-33_at_CHM.agr.jpg#mediaviewer/File:ASR-33_at_CHM.agr.jpg">
		ASR-33 at CHM.agr
	      </a> 
	      by 
	      <a href="//commons.wikimedia.org/wiki/User:ArnoldReinhold"
		 title="User:ArnoldReinhold">
		Arnold Reinhold
	      </a> 
	      Licensed under 
	      <a title="Creative Commons Attribution-Share Alike 3.0"
		 href="http://creativecommons.org/licenses/by-sa/3.0">
		CC BY-SA 3.0
	      </a> 
	      via 
	      <a href="//commons.wikimedia.org/wiki/">
		Wikimedia Commons
	      </a>.
	    </div>
	  </div>
	  <p>
	    The teletype machine dramatically changed the nature of
	    human computer interaction. Instead of the programmer
	    stringing together a sequence of operations and executing
	    that program on the computer, working with the computer
	    became an sequence of interactions. The user would type
	    something, then the computer could type something back to
	    the user.
	  </p>
	  <p>
	    For example, one could edit a file stored in memory by
	    listing a portion of the file. Then issuing commands to
	    change the contents of the memory. For example, the sample
	    below shows the command to list lines 20-30, then to
	    delete line 25. Then to alter line 26 by replacing a
	    "776" with "777".
	    <div class="code">
>l/20/30
20 C FURTHERMORE, THE SUM OF TWO SIDES OF A TRIANGLE
21 C IS GREATER THAN THE THIRD SIDE, SO WE CHECK FOR THAT, TOO
22       IF (IA) 777, 777, 701
23  701 IF (IB) 777, 777, 702
24  702 IF (IC) 777, 777, 703
25 OOPS BIG MISTAKE
26  703 IF (IA+IB-IC) 776,777,704
27  704 IF (IA+IC-IB) 777,777,705
28  705 IF (IB+IC-IA) 777,777,799
29  777 STOP 1
30 
d/25
a/26/776/777
	    </div>
	    To see the changes to the program, the programmer needs to
	    list the new contents of the file.
	  </p>
	  <p>
	    As clumsy as this seems, it was a great improvement over
	    earlier systems. The program is in Fortran II and the
	    programmer is altering the program by typing it directly
	    onto the computer. As you can see, one can now type in
	    things like (IA+IB-IC) to tell the computer to add the
	    contents of the memory location called A, interpreted as
	    an Integer to the memory location called B, also
	    interpreted as an integer, and then subtract the value in
	    memory location C, yet again interpreted as an
	    integer. Expressing this phrase in assembly language would
	    take four or five instructions. 
	  </p>
	  <p>
	    The terminal was the next step forwards in Human Computer
	    Interaction. The terminal replaces the roll of paper with
	    a Cathode Ray Tube (CRT) display. Initially, this was
	    simply an improved teletype: instead of printing the
	    characters on a roll of paper, it would display them on
	    the screen. It saved paper, but not much else. In fact,
	    it was a little inconvenient because interactions that
	    scrolled off the top of the screen were lost.
	  </p>
	  <p>
	    However, the CRT display's inability to save information
	    over the long run turned out to be a benefit. Because the
	    CRT needs to be rewritten sixty times a second, the
	    terminal can provide a window into the working of the
	    computer. Instead of getting a listing of a program, the
	    editor can change values in the memory locations shown on
	    the screen so when the screen is redrawn, it shows the new
	    value of the memory locations. The CRT based editor
	    improves interaction by showing the user how his or her
	    actions are actually changing the computer.
	  <p>
	  <p>
	    Terminals came into their own with the developement of
	    minicomputers. Minicomputers were build using Large Scale
	    Integration solid state transistors. The earlier
	    generation of computers had been build using either relays
	    or tubes and either Williamson tubes or core memories. All
	    of this technology was expensive and fragile. The machines
	    themselves were extremely expensive to build and required
	    teams of engineers to keep running. Minicomputers were
	    created using a new technology where the equivalent of
	    many tubes could be printed on a tiny silicon chip and the
	    chips wired together to create a computer. The chips were
	    much more reliable, cheaper and smaller than the earlier
	    technology. Minicomputers could easily support arrays of
	    terminals running command line interpreters. It was
	    becoming cheaper to have the machine do the work than to
	    have the people running the machine do the work.  
	  </p>
	  <h3>Command Line Interface</h3>
	  <p>
	    Interaction characterized by the user typing in a command
	    and the computer responding to that command is called a
	    Command Line Interface (CLI). It became possible only
	    after the development of time sharing because it was too
	    costly to dedicate a computer to respond to a single
	    person. 
	  </p>
	  <div class="illustration">
	    <img src="img/unix-terminal.jpg" alt="Teletype">
	    <div class="caption">
	      A Unix Terminal running Git.
	    </div>
	    <div class="imgref">
	      Nathaniel G. Martin
	    </div>
	  </div>
	  <p>
	    Command Line Interfaces still exist, though ever less
	    frequently. Above is one of the most common remaining
	    CLI: the Unix terminal. The advantages
	    of the CLI are:
	    <ol>
	      <li>
		It makes only light demands on the computer that is
		running it.
	      </li>
	      <li>
		It is very easy to write the program that executes the
		commands.
	      </li>
	      <li>
		It can be very fast for the expert user.
	      </li>
	    </ol>
	    The disadvantages of the CLI are:
	    <ol>
	      <li>
		It is difficult to learn
	      </li>
	      <li>
		It does not indicate what the user can do with it.
	      </li>
	      <li>
		The mnemonic devices availalbe are limited to what can
		be typed on the keyboard.
	      </li>
	    </ol>
	  </p>
	  <p>
	    Command Line Interfaces have dissappear primarily because
	    they are difficult to learn and remember. The limitations
	    of early hardware necessitated them, but computation is no
	    longer expensive. Modern interfaces try to reduce the
	    cognative burden on the computer user by using vast
	    quantities of computing power. When computing power is
	    limited, however, command line interfaces remain a choice.
	  </p>
	  <h3>References</h3>
	  <ul class="references">
	    <li class="ref">
	      <a href="http://en.wikipedia.org/wiki/History_of_software">
		History of Software
	      </a>
	    </li>
	    <li class="ref">
	      <a href="">

	      </a>
	    </li>
	  </ul>
	</div>
      </div>
    </div>

    <script src="/js/jquery.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/angular.min.js"></script>
    <script src="/js/wp.js"></script>
  </body>
</html>
